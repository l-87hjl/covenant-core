---
doc_type: primitive
tags: [interpretability, patterns]
status: draft
last_reviewed: 2026-01-20
---

# Interpretability: Known Patterns

## Patterns

1. **Saliency Maps**: Highlight important input regions
2. **Feature Visualization**: Generate inputs that maximally activate neurons
3. **Layer-wise Relevance Propagation**: Track influence backwards through network
4. **LIME/SHAP**: Local approximations of model behavior
5. **Concept Activation Vectors**: Test for high-level concept encoding
6. **Probing Classifiers**: Train simple models on representations
7. **Mechanistic Interpretability**: Reverse-engineer circuits in networks
8. **Chain-of-Thought**: Generate explicit reasoning steps

## See Also

- [README.md](README.md)
- [open_questions.md](open_questions.md)
